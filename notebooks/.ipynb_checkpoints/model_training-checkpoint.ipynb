{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c511c6-20a3-4dba-bc7c-b316d43c87b3",
   "metadata": {},
   "source": [
    "# **Model Training: Gas Turbina**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2462af47-6fc9-4894-8c8d-e04b5a469638",
   "metadata": {},
   "source": [
    "Since the EDA showed some non-linearity in Turbine Energy Yield (TEY) with respect to features, we will use models that capture these non-linearities.\n",
    "\n",
    "Models:\n",
    "\n",
    "- Randon Forests\n",
    "- XGboost\n",
    "- Neural Networks\n",
    "- Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe05b8e9-779e-480b-ad01-c120247272e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# data's 2011\n",
    "gt_2011 = pd.read_csv('../data/gas_turbine_emision/gt_2011.csv' )\n",
    "gt_2011['Year'] = 2011\n",
    "\n",
    "# data's 2012\n",
    "gt_2012 = pd.read_csv('../data/gas_turbine_emision/gt_2012.csv' )\n",
    "gt_2012['Year'] = 2014\n",
    "\n",
    "# data's 2013\n",
    "gt_2013 = pd.read_csv('../data/gas_turbine_emision/gt_2013.csv' )\n",
    "gt_2013['Year'] = 2013\n",
    "\n",
    "# data's 2014\n",
    "gt_2014 = pd.read_csv('../data/gas_turbine_emision/gt_2014.csv' )\n",
    "gt_2014['Year'] = 2014\n",
    "\n",
    "# data's 2015\n",
    "gt_2015 = pd.read_csv('../data/gas_turbine_emision/gt_2015.csv' )\n",
    "gt_2015['Year'] = 2015\n",
    "\n",
    "\n",
    "gt = pd.concat([gt_2011, gt_2012, gt_2013, gt_2014, gt_2015], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4c1e8-7c9d-46c1-ab6d-bb000fb9e412",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Random Forests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8599c832-bd82-48f2-9742-5d5bf283791f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6309406027072189\n",
      "R2 Score: 0.9983665528319152\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split data\n",
    "X = gt.drop(columns=['TEY'])\n",
    "y = gt['TEY']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "print('R2 Score:', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576fb9f-28d9-41b6-9abf-153de26a799e",
   "metadata": {},
   "source": [
    "### **XGboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e3906c1-1393-43e7-8e56-e66f41d7fde5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6778543264263197\n",
      "R2 Score: 0.9981146113675685\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Train model\n",
    "model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "print('R2 Score:', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb6c7b-23be-4aa2-884b-2c0e4051b5d4",
   "metadata": {},
   "source": [
    "### **SVR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "004ba3af-f2c0-4d8d-afc8-a4b0cc47c380",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.798293075699801\n",
      "R2 Score: 0.32790255889491626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Train model\n",
    "model = SVR(kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "print('R2 Score:', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e37664-4647-4922-a42c-6b635910c58d",
   "metadata": {},
   "source": [
    "### **Neural Network**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f75cd149-8611-42a5-9701-fb11972cb485",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 15.9185\n",
      "Epoch [2/50], Loss: 15.7784\n",
      "Epoch [3/50], Loss: 11.2145\n",
      "Epoch [4/50], Loss: 7.8605\n",
      "Epoch [5/50], Loss: 4.5186\n",
      "Epoch [6/50], Loss: 3.3040\n",
      "Epoch [7/50], Loss: 2.8145\n",
      "Epoch [8/50], Loss: 2.1687\n",
      "Epoch [9/50], Loss: 3.2363\n",
      "Epoch [10/50], Loss: 2.7823\n",
      "Epoch [11/50], Loss: 2.1711\n",
      "Epoch [12/50], Loss: 1.8753\n",
      "Epoch [13/50], Loss: 1.2740\n",
      "Epoch [14/50], Loss: 2.4648\n",
      "Epoch [15/50], Loss: 2.3358\n",
      "Epoch [16/50], Loss: 0.9906\n",
      "Epoch [17/50], Loss: 1.3737\n",
      "Epoch [18/50], Loss: 1.4369\n",
      "Epoch [19/50], Loss: 1.4444\n",
      "Epoch [20/50], Loss: 1.8640\n",
      "Epoch [21/50], Loss: 1.3789\n",
      "Epoch [22/50], Loss: 1.6881\n",
      "Epoch [23/50], Loss: 1.4980\n",
      "Epoch [24/50], Loss: 1.4698\n",
      "Epoch [25/50], Loss: 2.0576\n",
      "Epoch [26/50], Loss: 1.5076\n",
      "Epoch [27/50], Loss: 1.4967\n",
      "Epoch [28/50], Loss: 1.9779\n",
      "Epoch [29/50], Loss: 1.5673\n",
      "Epoch [30/50], Loss: 1.1279\n",
      "Epoch [31/50], Loss: 1.3035\n",
      "Epoch [32/50], Loss: 1.6537\n",
      "Epoch [33/50], Loss: 1.6817\n",
      "Epoch [34/50], Loss: 1.1341\n",
      "Epoch [35/50], Loss: 2.4853\n",
      "Epoch [36/50], Loss: 1.2592\n",
      "Epoch [37/50], Loss: 1.5846\n",
      "Epoch [38/50], Loss: 1.4686\n",
      "Epoch [39/50], Loss: 1.7071\n",
      "Epoch [40/50], Loss: 2.0650\n",
      "Epoch [41/50], Loss: 1.9446\n",
      "Epoch [42/50], Loss: 2.0790\n",
      "Epoch [43/50], Loss: 2.0863\n",
      "Epoch [44/50], Loss: 1.2226\n",
      "Epoch [45/50], Loss: 1.1867\n",
      "Epoch [46/50], Loss: 1.1336\n",
      "Epoch [47/50], Loss: 2.1494\n",
      "Epoch [48/50], Loss: 2.1892\n",
      "Epoch [49/50], Loss: 1.5012\n",
      "Epoch [50/50], Loss: 1.1810\n",
      "RMSE: 2.4021336939899434\n",
      "R2 Score: 0.9763232281310391\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming train and test are Pandas DataFrames\n",
    "# Example: train and test have columns 'feature1', 'feature2', ..., 'target'\n",
    "\n",
    "# Separate features (X) and target (y) for train and test\n",
    "X_train_np = X_train.values  # Convert DataFrame to numpy array\n",
    "y_train_np = y_train.values\n",
    "X_test_np = X_test.values\n",
    "y_test_np = y_test.values\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32)\n",
    "\n",
    "# Create a DataLoader for training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the Neural Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)  # Input layer to hidden layer (64 neurons)\n",
    "        self.fc2 = nn.Linear(64, 32)         # Hidden layer (64 neurons) to hidden layer (32 neurons)\n",
    "        self.fc3 = nn.Linear(32, 1)          # Hidden layer (32 neurons) to output layer (1 neuron)\n",
    "        self.relu = nn.ReLU()                # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation for regression output\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "model = SimpleNN(input_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Root Mean Squared Error Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam optimizer\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = torch.sqrt( criterion(outputs, batch_y.unsqueeze(1)) )  # Add extra dimension to batch_y\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for each epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "    y_pred = y_pred_tensor.numpy()  # Convert predictions to numpy array\n",
    "\n",
    "# Calculate RMSE and R2 Score\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('RMSE:', rmse)\n",
    "print('R2 Score:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff99a7-66fd-43d0-9f78-5fc9f45460ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06c95d44-bc55-4bf9-a9ec-220d13e19106",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 12.7055\n",
      "Epoch [2/50], Loss: 11.1755\n",
      "Epoch [3/50], Loss: 14.5827\n",
      "Epoch [4/50], Loss: 14.9308\n",
      "Epoch [5/50], Loss: 11.1173\n",
      "Epoch [6/50], Loss: 11.2879\n",
      "Epoch [7/50], Loss: 8.4283\n",
      "Epoch [8/50], Loss: 9.0800\n",
      "Epoch [9/50], Loss: 6.9546\n",
      "Epoch [10/50], Loss: 3.8544\n",
      "Epoch [11/50], Loss: 3.8431\n",
      "Epoch [12/50], Loss: 2.3537\n",
      "Epoch [13/50], Loss: 2.7514\n",
      "Epoch [14/50], Loss: 1.8905\n",
      "Epoch [15/50], Loss: 2.6046\n",
      "Epoch [16/50], Loss: 1.8857\n",
      "Epoch [17/50], Loss: 1.7869\n",
      "Epoch [18/50], Loss: 1.7176\n",
      "Epoch [19/50], Loss: 1.8138\n",
      "Epoch [20/50], Loss: 2.1267\n",
      "Epoch [21/50], Loss: 1.6524\n",
      "Epoch [22/50], Loss: 1.1641\n",
      "Epoch [23/50], Loss: 2.5789\n",
      "Epoch [24/50], Loss: 1.9459\n",
      "Epoch [25/50], Loss: 0.7264\n",
      "Epoch [26/50], Loss: 1.3648\n",
      "Epoch [27/50], Loss: 3.0549\n",
      "Epoch [28/50], Loss: 1.9294\n",
      "Epoch [29/50], Loss: 1.4346\n",
      "Epoch [30/50], Loss: 1.9572\n",
      "Epoch [31/50], Loss: 1.5490\n",
      "Epoch [32/50], Loss: 2.4520\n",
      "Epoch [33/50], Loss: 1.5640\n",
      "Epoch [34/50], Loss: 1.2576\n",
      "Epoch [35/50], Loss: 1.0150\n",
      "Epoch [36/50], Loss: 1.3525\n",
      "Epoch [37/50], Loss: 1.2065\n",
      "Epoch [38/50], Loss: 1.1358\n",
      "Epoch [39/50], Loss: 1.2577\n",
      "Epoch [40/50], Loss: 1.6089\n",
      "Epoch [41/50], Loss: 1.3457\n",
      "Epoch [42/50], Loss: 2.1248\n",
      "Epoch [43/50], Loss: 2.4325\n",
      "Epoch [44/50], Loss: 1.0927\n",
      "Epoch [45/50], Loss: 1.3105\n",
      "Epoch [46/50], Loss: 1.0759\n",
      "Epoch [47/50], Loss: 1.2809\n",
      "Epoch [48/50], Loss: 1.4502\n",
      "Epoch [49/50], Loss: 1.3816\n",
      "Epoch [50/50], Loss: 0.9953\n",
      "RMSE: 1.5921502\n",
      "R2 Score: 0.9895984944970988\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Separate features (X) and target (y) for train and test\n",
    "X_train_np = X_train.values  # Convert DataFrame to numpy array\n",
    "y_train_np = y_train.values\n",
    "X_test_np = X_test.values\n",
    "y_test_np = y_test.values\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "# Define model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)  # Normalización Batch\n",
    "        self.dropout1 = nn.Dropout(0.05)  # Dropout de 20%\n",
    "\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)  # Normalización Batch\n",
    "        self.dropout2 = nn.Dropout(0.1)  # Dropout de 20%\n",
    "\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(  self.fc1(x)  )\n",
    "        # x = self.dropout1(x)\n",
    "        x = torch.relu( self.fc2(x)   ) \n",
    "        # x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleModel(X_train.shape[1])\n",
    "\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(nn.MSELoss()(y_pred, y_true) + 1e-8)\n",
    "\n",
    "criterion = RMSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train model\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_X)\n",
    "        loss = criterion(y_pred, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    \n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy()\n",
    "\n",
    "y_test_np = y_test_tensor.numpy()\n",
    "print('RMSE:', mean_squared_error(y_test_np, y_pred, squared=False))\n",
    "print('R2 Score:', r2_score(y_test_np, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e89365-015e-4425-b3fa-ab74b9176277",
   "metadata": {},
   "source": [
    "### **Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65bacd9a-b458-4c29-ad00-83f91b737b38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9526760174890128\n",
      "R2 Score: 0.9962759234057789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "print('R2 Score:', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f22208-6501-4468-a80c-b9f04699ad63",
   "metadata": {},
   "source": [
    "**Implementation Neural Network Sequetial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eda2542a-560b-45ba-b2d4-e956e3326e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 9.3605\n",
      "Epoch [2/30], Loss: 16.4405\n",
      "Epoch [3/30], Loss: 14.2831\n",
      "Epoch [4/30], Loss: 6.5444\n",
      "Epoch [5/30], Loss: 6.0547\n",
      "Epoch [6/30], Loss: 7.1938\n",
      "Epoch [7/30], Loss: 4.1058\n",
      "Epoch [8/30], Loss: 3.8899\n",
      "Epoch [9/30], Loss: 2.7259\n",
      "Epoch [10/30], Loss: 3.1827\n",
      "Epoch [11/30], Loss: 2.1833\n",
      "Epoch [12/30], Loss: 1.6510\n",
      "Epoch [13/30], Loss: 1.5384\n",
      "Epoch [14/30], Loss: 2.7545\n",
      "Epoch [15/30], Loss: 1.8946\n",
      "Epoch [16/30], Loss: 2.9216\n",
      "Epoch [17/30], Loss: 1.9433\n",
      "Epoch [18/30], Loss: 4.5035\n",
      "Epoch [19/30], Loss: 2.0331\n",
      "Epoch [20/30], Loss: 3.1704\n",
      "Epoch [21/30], Loss: 1.5661\n",
      "Epoch [22/30], Loss: 2.5183\n",
      "Epoch [23/30], Loss: 1.4569\n",
      "Epoch [24/30], Loss: 1.4632\n",
      "Epoch [25/30], Loss: 1.3138\n",
      "Epoch [26/30], Loss: 1.3630\n",
      "Epoch [27/30], Loss: 2.2453\n",
      "Epoch [28/30], Loss: 1.5302\n",
      "Epoch [29/30], Loss: 2.0054\n",
      "Epoch [30/30], Loss: 2.5792\n",
      "Epoch [31/30], Loss: 2.0569\n",
      "Epoch [32/30], Loss: 2.0166\n",
      "Epoch [33/30], Loss: 0.9535\n",
      "Epoch [34/30], Loss: 2.1010\n",
      "Epoch [35/30], Loss: 1.6253\n",
      "Epoch [36/30], Loss: 1.2705\n",
      "Epoch [37/30], Loss: 1.3467\n",
      "Epoch [38/30], Loss: 1.4658\n",
      "Epoch [39/30], Loss: 1.7647\n",
      "Epoch [40/30], Loss: 1.4401\n",
      "Epoch [41/30], Loss: 1.7221\n",
      "Epoch [42/30], Loss: 1.7783\n",
      "Epoch [43/30], Loss: 2.3897\n",
      "Epoch [44/30], Loss: 1.3775\n",
      "Epoch [45/30], Loss: 2.4306\n",
      "Epoch [46/30], Loss: 1.2054\n",
      "Epoch [47/30], Loss: 1.5537\n",
      "Epoch [48/30], Loss: 1.7887\n",
      "Epoch [49/30], Loss: 1.6468\n",
      "Epoch [50/30], Loss: 1.4199\n",
      "RMSE: 1.548957\n",
      "R2 Score: 0.9901551998065325\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Separate features (X) and target (y) for train and test\n",
    "X_train_np = X_train.values  # Convert DataFrame to numpy array\n",
    "y_train_np = y_train.values\n",
    "X_test_np = X_test.values\n",
    "y_test_np = y_test.values\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "# Define model using Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 64),\n",
    "   # nn.BatchNorm1d(64),\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(nn.MSELoss()(y_pred, y_true))\n",
    "\n",
    "criterion = RMSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(50):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_X)\n",
    "        loss = criterion(y_pred, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy()\n",
    "\n",
    "y_test_np = y_test_tensor.numpy()\n",
    "print('RMSE:', mean_squared_error(y_test_np, y_pred, squared=False))\n",
    "print('R2 Score:', r2_score(y_test_np, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23048a69-dedf-4cc6-bac3-960d7c20fe06",
   "metadata": {},
   "source": [
    "### **Ensemble Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "845b7921-078d-4d38-9242-560ba29a0d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6538504200832819\n",
      "R2 Score: 0.9982457763818717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define models\n",
    "model1 = LinearRegression()\n",
    "model2 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model3 = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Create ensemble\n",
    "ensemble = VotingRegressor(estimators=[('lr', model1), ('rf', model2), ('xgb', model3)])\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = ensemble.predict(X_test)\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "print('R2 Score:', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35890e9c-bacc-4a70-a0ab-d20ec74960be",
   "metadata": {},
   "source": [
    "The best implementation are Random Forest and XGboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
